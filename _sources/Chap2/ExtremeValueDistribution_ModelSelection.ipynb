{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0a4394-f393-483f-be77-57092dcc23f7",
   "metadata": {},
   "source": [
    "# Chapter 2 Hydrology: Flood Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf62ca6-9c4f-4646-842b-d3d6a915b963",
   "metadata": {},
   "source": [
    "1. [Introduction: Flood frequency](#1.-Introduction)\n",
    "2. [Simulation: EVD performance](#2.-Simulation)\n",
    "3. [Self-Assessment](#3.-Self-Assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9ceb65-e216-441e-a38d-13e59bc17273",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd24ad-3288-47b8-bf05-a1a8b82b8c66",
   "metadata": {},
   "source": [
    "## üåä Flood Frequency Analysis\n",
    "\n",
    "Flood Frequency Analysis (FFA) is a statistical method used to estimate the probability of different magnitudes of flood events occurring at a specific location over time {cite:p}`england2018bulletin17c, holland2016ffa, cunnane1989flood`.\n",
    "\n",
    "### üîç Key Concepts\n",
    "\n",
    "- **Annual Maximum Series (AMS):** Uses the highest flow recorded each year.\n",
    "- **Return Period (T):** Average interval between floods of a given magnitude.\n",
    "- **Exceedance Probability (p):**  \n",
    "  $$ p = \\frac{1}{T} $$  \n",
    "  where $T$ is the return period.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Importance of Observational Data\n",
    "\n",
    "Observational data are crucial for conducting reliable flood frequency analyses.\n",
    "\n",
    "### ‚úÖ Benefits:\n",
    "- Long-term records improve statistical reliability.\n",
    "- Site-specific data reflect local hydrologic behavior.\n",
    "- Empirical distributions can be fitted directly.\n",
    "\n",
    "### ‚ö†Ô∏è Limitations with Sparse Data:\n",
    "- High uncertainty in flood quantile estimates.\n",
    "- Poor representation of rare, extreme events.\n",
    "- Skewness and variance may be biased.\n",
    "\n",
    "---\n",
    "\n",
    "### üìê Theoretical Distributions for Extreme Events\n",
    "\n",
    "When data are limited, theoretical distributions help extrapolate flood probabilities.\n",
    "\n",
    "### üß† Common Distributions\n",
    "\n",
    "| **Distribution** | **Type** | **Use Case** | **Notes** |\n",
    "|------------------|----------|--------------|-----------|\n",
    "| **Gumbel (EV1)** | Extreme Value | AMS floods | Assumes exponential tail |\n",
    "| **Log-Pearson Type III** | Skewed | US standard (Bulletin 17C) | Handles skewness well |\n",
    "| **Generalized Extreme Value (GEV)** | Flexible | Global use | Includes Gumbel, Fr√©chet, Weibull |\n",
    "| **Weibull** | Empirical | Plotting positions | Used for ranking floods |\n",
    "\n",
    "\n",
    "### üìä Empirical vs. Theoretical Distributions in Extreme Value Analysis\n",
    "\n",
    "\n",
    "### üîç Empirical Distribution\n",
    "\n",
    "- **Definition**: Based directly on observed data (e.g., annual peak discharges).\n",
    "- **Construction**: Uses ranking and plotting positions (e.g., Weibull formula) to estimate exceedance probabilities.\n",
    "- **Strengths**:\n",
    "  - Reflects site-specific behavior.\n",
    "  - No assumptions about underlying distribution.\n",
    "- **Limitations**:\n",
    "  - Requires long-term data for reliability.\n",
    "  - Poor extrapolation for rare/extreme events.\n",
    "  - Sensitive to outliers and sampling variability.\n",
    "\n",
    "---\n",
    "\n",
    "### üìê Theoretical Distribution\n",
    "\n",
    "- **Definition**: Assumes data follow a known probability distribution (e.g., Gumbel, Log-Pearson III, GEV).\n",
    "- **Construction**: Fits a parametric model to data using methods like Maximum Likelihood Estimation (MLE).\n",
    "- **Strengths**:\n",
    "  - Enables extrapolation beyond observed range.\n",
    "  - Provides analytical expressions for return periods and quantiles.\n",
    "- **Limitations**:\n",
    "  - Requires assumptions about distribution type.\n",
    "  - Sensitive to parameter estimation, especially with sparse data.\n",
    "\n",
    "---\n",
    "\n",
    "### üåä Why Use Theoretical Distributions for Extreme Events?\n",
    "\n",
    "Extreme events (e.g., 100-year floods) are rare and often absent from short observational records. Theoretical distributions allow us to estimate their magnitude and frequency by modeling the tail behavior of the data.\n",
    "\n",
    "### üìà Most Popular Distributions for Extremes\n",
    "\n",
    "| **Distribution** | **Type** | **Notes** |\n",
    "|------------------|----------|-----------|\n",
    "| **Generalized Extreme Value (GEV)** | Block maxima | Unifies Gumbel, Fr√©chet, Weibull; widely used globally |\n",
    "| **Log-Pearson Type III** | Skewed | Official standard in the U.S. (Bulletin 17C) |\n",
    "| **Gumbel (EV1)** | Light-tailed | Historically common for flood peaks |\n",
    "\n",
    "---\n",
    "\n",
    "### üåê Regional Flood Frequency Analysis (RFFA)\n",
    "\n",
    "Used when site-specific data are sparse or unavailable.\n",
    "\n",
    "### üó∫Ô∏è Key Features:\n",
    "- **Homogeneous Region**: Group sites with similar hydrologic characteristics.\n",
    "- **Index Flood Method**: Normalize data by a regional index (e.g., mean annual flood).\n",
    "- **Regional Regression**: Use basin attributes to estimate flood quantiles.\n",
    "- **L-Moments or Bayesian Methods**: Improve parameter estimation across sites.\n",
    "\n",
    "---\n",
    "\n",
    "### üåç Most Widely Used:\n",
    "- **United States:** Log-Pearson Type III (per USGS Bulletin 17C)\n",
    "- **Globally:** Generalized Extreme Value (GEV)\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Hybrid Approaches\n",
    "\n",
    "When observational data are sparse:\n",
    "\n",
    "- **Bayesian Estimation:** Combines prior knowledge with observed data.\n",
    "- **Regional Skew Adjustment:** Blends site-specific and regional skew.\n",
    "- **Expected Moments Algorithm (EMA):** Handles censored and interval data.\n",
    "\n",
    "---\n",
    "\n",
    "### üåê Regional Flood Frequency Analysis (RFFA)\n",
    "\n",
    "Used when site-specific data are insufficient.\n",
    "\n",
    "### üó∫Ô∏è Key Steps:\n",
    "1. **Define Homogeneous Region:** Based on climate, physiography, land use.\n",
    "2. **Index Flood Method:** Normalize peak flows by a regional index.\n",
    "3. **Regional Regression Models:** Use basin characteristics to estimate flood quantiles.\n",
    "4. **L-Moments or GAMLSS:** For parameter estimation across sites.\n",
    "\n",
    "### üìò USGS Practice:\n",
    "- Uses regional skewness maps and regression equations.\n",
    "- Bulletin 17C recommends combining at-site and regional skew.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Summary\n",
    "\n",
    "Flood frequency analysis is essential for infrastructure design and risk management. While observational data are ideal, theoretical distributions‚Äîespecially GEV and Log-Pearson Type III‚Äîenable extrapolation when data are sparse. Regional methods fill gaps for ungauged sites, ensuring robust flood risk estimates.\n",
    "\n",
    "### Foundational Literature\n",
    "\n",
    "Flood Frequency Analysis (FFA) is a statistical method used to estimate the probability of different magnitudes of flood events occurring at a specific location over time and {cite:p}`england2018bulletin17c, holland2016ffa, cunnane1989flood` provides foundational knowledge on extreme FFA. These works collectively establish the statistical frameworks, methodological advancements, and practical guidelines for estimating the probability of flood events of varying magnitudes at specific locations over time. {cite}`cunnane1989flood` provides a comprehensive review of statistical distributions used in FFA, including selection criteria for distribution, and serves as a global reference for hydrologists conducting design flood estimation. {cite}`holland2016ffa`- Evaluates the relevance of at-site FFA methods for extreme events using a simulation-based approach to improve reliability in estimating extreme flood probabilities. {cite}`england2018bulletin17c` is an update to Bulletin 17B; it addresses challenges in estimating rare floods and supports risk-informed design, and represents the U.S. national standard for flood frequency estimation. Together, these sources form the core methodological and practical basis for assessing extreme flood risk and designing infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec99960-92ba-4cf7-b9b2-c81f39b45b25",
   "metadata": {},
   "source": [
    "### Flood frequency analysis tool\n",
    "\n",
    "This tool reads peak flow data from the USGS NWIS database and fits 10 commonly used extreme value probability distributions to estimate flood magnitudes associated with various return periods (e.g., 2-year, 100-year). It performs statistical goodness-of-fit evaluation and provides an interactive interface to visualize the flood frequency curve for each distribution.\n",
    "\n",
    "---\n",
    "\n",
    "###  What the Tool Does\n",
    "\n",
    "- ‚úÖ Reads annual peak discharge data from a NWIS `.txt` file\n",
    "- ‚úÖ Fits multiple statistical distributions to the observed peak flows\n",
    "- ‚úÖ Computes estimated flood quantiles for specific return periods (2, 5, 10, 25, 50, 100 years)\n",
    "- ‚úÖ Calculates RMSE and Kolmogorov‚ÄìSmirnov (KS) goodness-of-fit metrics\n",
    "- ‚úÖ Allows the user to interactively select a distribution and view:\n",
    "  - Estimated peak flows\n",
    "  - Distribution parameters\n",
    "  - GOF statistics\n",
    "  - A flood frequency curve plotted in log scale\n",
    "\n",
    "---\n",
    "\n",
    "###  How to Use\n",
    "\n",
    "1. **Prepare Input File**  \n",
    "   - Download annual peak streamflow data from the [USGS NWIS Peak Flow site](https://waterdata.usgs.gov/nwis/peak)\n",
    "   - Save as a tab-delimited `.txt` file (e.g., `07022500_nwis_peak.txt`)\n",
    "\n",
    "2. **Run the Script in Jupyter Notebook**\n",
    "   - Place the file in your working directory\n",
    "   - Modify the line `usgs_file = \"07022500_nwis_peak.txt\"` to match your filename\n",
    "   - Run the script cell-by-cell\n",
    "\n",
    "3. **Explore Results**\n",
    "   - View the summary table of fitted distribution parameters and their statistical performance\n",
    "   - Use the dropdown selector to compare estimated flood flows and curves for each distribution\n",
    "\n",
    "---\n",
    "\n",
    "### Theoretical Background: Distributions Used\n",
    "\n",
    "Each distribution estimates the probability of rare flood events based on historical data. Here's a quick reference:\n",
    "\n",
    "| Distribution           | Description                                                                 | Parameters                        |\n",
    "|------------------------|-----------------------------------------------------------------------------|-----------------------------------|\n",
    "| **Gumbel (EV1)**        | Models block maxima (e.g., annual max). Skewed right.                      | Location (Œº), Scale (Œ≤)           |\n",
    "| **Log-Pearson III**     | Log-transformed Pearson Type III. Used in U.S. federal flood studies.      | Shape (Œ±), Location (Œº), Scale    |\n",
    "| **GEV**                 | General form for extremes. Includes Gumbel, Frechet, Weibull as cases.     | Shape (Œæ), Location, Scale        |\n",
    "| **Normal**              | Symmetric bell curve. May misrepresent skewed flood data.                  | Mean (Œº), Std. dev. (œÉ)           |\n",
    "| **Lognormal**           | Data is normally distributed after log transform. Skewed right.            | Shape (œÉ), Location, Scale        |\n",
    "| **Weibull (Type III)**  | Useful for extreme minimums or upper tails.                                | Shape (k), Location, Scale        |\n",
    "| **Exponential**         | Special case of Weibull; constant failure rate (rarely used for floods).   | Rate (Œª) or Scale                 |\n",
    "| **Gamma**               | General skewed distribution, flexible fit for hydrology                    | Shape (k), Scale (Œ∏), Location    |\n",
    "| **Loglogistic (Fisk)**  | Skewed right, like lognormal but heavier tail.                             | Shape (c), Location, Scale        |\n",
    "| **Generalized Pareto**  | Models excesses over a threshold (POT approach).                           | Shape, Location, Scale            |\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Evaluation Criteria\n",
    "\n",
    "Two statistical metrics assess how well each distribution fits the observed data:\n",
    "\n",
    "- ### üîπ Root Mean Squared Error (RMSE)\n",
    "  Measures average error between observed peak flows and estimated quantiles from the distribution:\n",
    "  $$\n",
    "  \\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum (Q_{\\text{obs}} - Q_{\\text{est}})^2 }\n",
    "  $$\n",
    "  Lower values indicate a better fit.\n",
    "\n",
    "- ### üîπ Kolmogorov‚ÄìSmirnov (KS) Statistic\n",
    "  Measures the maximum difference between the empirical cumulative distribution function (ECDF) and the theoretical CDF:\n",
    "  $$\n",
    "  D = \\sup_x |F_n(x) - F(x)|\n",
    "  $$\n",
    "  - Returns both the **KS statistic** and a **p-value**\n",
    "  - If p-value > 0.05: distribution is a statistically valid fit (‚úÖ Pass)\n",
    "\n",
    "---\n",
    "\n",
    "### Output Summary\n",
    "\n",
    "- A sorted summary table of all distributions including:\n",
    "  - Fitted parameters\n",
    "  - RMSE\n",
    "  - KS statistic and p-value\n",
    "  - Pass/fail interpretation\n",
    "- Interactive flood frequency plots for return periods on a log-x axis\n",
    "- Ability to choose which distribution best represents the dataset\n",
    "\n",
    "---\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Floodplain mapping\n",
    "- Hydraulic structure design (culverts, bridges, dams)\n",
    "- Return period‚Äìbased risk estimation\n",
    "- Hydrologic modeling calibration\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like this tool extended with confidence intervals, percentile shading, or exported reports in Excel or PDF!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c87f8-1730-4016-8b01-f7ea2fa1e40c",
   "metadata": {},
   "source": [
    "## 2. Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00cb1698-bce1-420c-bccd-c57addd2bc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_cd   site_no peak_dt peak_va</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USGS    7022500 3/3/1953    780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USGS    7022500 1/20/1954   520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USGS    7022500 3/20/1955   846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USGS    7022500 2/2/1956    440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USGS    7022500 1/22/1957   707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  agency_cd   site_no peak_dt peak_va\n",
       "0     USGS    7022500 3/3/1953    780\n",
       "1     USGS    7022500 1/20/1954   520\n",
       "2     USGS    7022500 3/20/1955   846\n",
       "3     USGS    7022500 2/2/1956    440\n",
       "4     USGS    7022500 1/22/1957   707"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'peak_va'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'peak_va'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m\n\u001b[0;32m     47\u001b[0m display(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Display the first few rows\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m#df.head()\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28mprint\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpeak_va\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'peak_va'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Simulate the file content as a string\n",
    "data = \"\"\"\\\n",
    "agency_cd   site_no peak_dt peak_va\n",
    "USGS    7022500 3/3/1953    780\n",
    "USGS    7022500 1/20/1954   520\n",
    "USGS    7022500 3/20/1955   846\n",
    "USGS    7022500 2/2/1956    440\n",
    "USGS    7022500 1/22/1957   707\n",
    "USGS    7022500 11/18/1957  763\n",
    "USGS    7022500 8/6/1959    514\n",
    "USGS    7022500 7/3/1960    602\n",
    "USGS    7022500 3/12/1961   304\n",
    "USGS    7022500 9/16/1962   833\n",
    "USGS    7022500 3/4/1963    228\n",
    "USGS    7022500 3/4/1964    690\n",
    "USGS    7022500 3/29/1965   1870\n",
    "USGS    7022500 3/20/1968   545\n",
    "USGS    7022500 4/9/1969    702\n",
    "USGS    7022500 6/13/1970   295\n",
    "USGS    7022500 8/21/1971   987\n",
    "USGS    7022500 7/28/1972   350\n",
    "USGS    7022500 5/1/1973    816\n",
    "USGS    7022500 11/24/1973  900\n",
    "USGS    7022500 3/29/1975   1160\n",
    "USGS    7022500 2/17/1976   800\n",
    "USGS    7022500 6/26/1977   748\n",
    "USGS    7022500 3/14/1978   728\n",
    "USGS    7022500 12/3/1978   1460\n",
    "USGS    7022500 3/17/1980   364\n",
    "USGS    7022500 6/6/1981    975\n",
    "USGS    7022500 1/4/1982    1080\n",
    "USGS    7022500 6/3/1983    2760\n",
    "USGS    7022500 4/29/1984   954\n",
    "USGS    7022500 9/5/1985    660\n",
    "USGS    7022500 5/24/1986   360\n",
    "USGS    7022500 2/28/1987   415\n",
    "\"\"\"\n",
    "\n",
    "# Read it as a DataFrame using tab separator\n",
    "df = pd.read_csv(StringIO(data), sep=\"\\t\")\n",
    "#df = pd.read_csv(StringIO(data), delim_whitespace=True)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "display(df.head())\n",
    "# Display the first few rows\n",
    "#df.head()\n",
    "\n",
    "print(df['peak_va'])\n",
    "\n",
    "#peak_df[\"peak_va\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643db02-d399-40cf-99b6-696997476e8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": [
    "### Required Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import kstest\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from ipywidgets import interact, Dropdown\n",
    "from IPython.display import display\n",
    "#pip install scipy\n",
    "### Load NWIS Peak Flow Data\n",
    "# def read_nwis_peak_file(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r') as f:\n",
    "#             lines = f.readlines()\n",
    "#         start_line = next(i for i, line in enumerate(lines) if not line.startswith('#'))\n",
    "#         df = pd.read_csv(\n",
    "#             file_path,\n",
    "#             sep='\\t',\n",
    "#             comment='#',\n",
    "#             header=0,\n",
    "#             dtype=str,\n",
    "#             engine='python'\n",
    "#         )\n",
    "#         df.columns = df.columns.str.strip()\n",
    "#         df['peak_dt'] = pd.to_datetime(df['peak_dt'], errors='coerce')\n",
    "#         df['peak_va'] = pd.to_numeric(df['peak_va'], errors='coerce')\n",
    "#         df_clean = df[['site_no', 'peak_dt', 'peak_va']].dropna()\n",
    "#         return df_clean\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error reading file: {e}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "### Load & Preview Data\n",
    "# usgs_file = \"07022500_nwis_peak.txt\"\n",
    "# peak_df = read_nwis_peak_file(usgs_file)\n",
    "# display(peak_df.head())\n",
    "\n",
    "### Set Up Distribution Parameters\n",
    "distributions = {\n",
    "    \"Gumbel (EV1)\": stats.gumbel_r,\n",
    "    \"Log-Pearson III\": stats.pearson3,\n",
    "    \"GEV\": stats.genextreme,\n",
    "    \"Normal\": stats.norm,\n",
    "    \"Lognormal\": stats.lognorm,\n",
    "    \"Weibull\": stats.weibull_min,\n",
    "    \"Exponential\": stats.expon,\n",
    "    \"Gamma\": stats.gamma,\n",
    "    \"Loglogistic\": stats.fisk,\n",
    "    \"Generalized Pareto\": stats.genpareto\n",
    "}\n",
    "\n",
    "###  Define Probability Array\n",
    "#peak_values = peak_df['peak_va'] #peak_df['peak_va'].dropna()\n",
    "peak_values=df['peak_va']\n",
    "sorted_data = np.sort(peak_values)\n",
    "prob_plot = np.linspace(0.01, 0.99, 100)\n",
    "return_periods = 1 / (1 - prob_plot)\n",
    "prob_exceed = 0.01  # For 100-year flood estimate\n",
    "\n",
    "###  Fit Distributions\n",
    "summary_rows = []\n",
    "fit_results = {}\n",
    "\n",
    "for name, dist in distributions.items():\n",
    "    try:\n",
    "        params = dist.fit(peak_values)\n",
    "        flood_q = dist.ppf(1 - prob_exceed, *params)\n",
    "#        q_estimates = dist.ppf(prob_plot, *params)\n",
    "        # Generate quantiles from same size as data\n",
    "        prob_plot = np.linspace(0.01, 0.99, len(sorted_data))  # now it's 33 points\n",
    "        q_estimates = dist.ppf(prob_plot, *params)\n",
    "\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(sorted_data, q_estimates))\n",
    "        ks_stat, ks_pval = kstest(peak_values, dist.cdf, args=params)\n",
    "\n",
    "        fit_results[name] = {\n",
    "            \"params\": params,\n",
    "            \"q\": dist.ppf(1 - 1 / return_periods, *params),\n",
    "            \"rmse\": rmse,\n",
    "            \"ks_stat\": ks_stat,\n",
    "            \"ks_pval\": ks_pval\n",
    "        }\n",
    "\n",
    "        param_str = \", \".join([f\"{p:.2f}\" for p in params])\n",
    "        summary_rows.append({\n",
    "            \"Distribution\": name,\n",
    "            \"Parameters\": param_str,\n",
    "            \"RMSE (cfs)\": round(rmse, 2),\n",
    "            \"KS Stat\": round(ks_stat, 3),\n",
    "            \"KS p-value\": round(ks_pval, 3),\n",
    "            \"KS Result\": \"‚úÖ Pass\" if ks_pval > 0.05 else \"‚ùå Reject\"\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not fit {name}: {e}\")\n",
    "\n",
    "### Summary Table\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(by=\"RMSE (cfs)\")\n",
    "print(\"\\nüìä Goodness-of-Fit Summary for All Distributions:\\n\")\n",
    "display(summary_df)\n",
    "\n",
    "### Interactive Plotting\n",
    "def plot_selected_distribution(dist_name):\n",
    "    result = fit_results[dist_name]\n",
    "    q = result[\"q\"]\n",
    "    params = result[\"params\"]\n",
    "    param_str = \", \".join([f\"{p:.2f}\" for p in params])\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(return_periods, q, marker='o', linestyle='-', color='royalblue', label=\"Estimated Peak Flow\")\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"Return Period (years, log scale)\")\n",
    "    plt.ylabel(\"Estimated Peak Flow (cfs)\")\n",
    "    plt.title(f\"{dist_name} Flood Frequency Curve\\nParameters: {param_str}\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Tabular Output\n",
    "    df_plot = pd.DataFrame({\n",
    "        \"Return Period (yr)\": return_periods.round(1),\n",
    "        \"Estimated Peak Flow (cfs)\": q.round(2)\n",
    "    })\n",
    "    print(f\"\\nüìå Parameters: {param_str}\")\n",
    "    print(f\"RMSE: {result['rmse']:.2f}, KS stat: {result['ks_stat']:.3f}, p-value: {result['ks_pval']:.3f}\")\n",
    "    display(df_plot)\n",
    "\n",
    "### Launch Widget\n",
    "interact(plot_selected_distribution, dist_name=Dropdown(options=list(fit_results.keys()), description=\"Distribution\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb9549-b350-4a4a-8183-0883c732983a",
   "metadata": {},
   "source": [
    "## 3. Self-Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47ac05-66f3-4001-9f83-66350098ecbf",
   "metadata": {},
   "source": [
    "### Self-Assessment: Flood Frequency Analysis Tool\n",
    "\n",
    "Use these prompts and questions to evaluate your understanding of the tool and its underlying hydrologic and statistical concepts.\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Questions\n",
    "\n",
    "1. **Why are return periods plotted on a logarithmic scale in flood frequency analysis?**\n",
    "   - *Hint: Think about how frequent vs. rare events are distributed.*\n",
    "\n",
    "2. **What is the purpose of fitting multiple distributions to the same peak flow dataset?**\n",
    "   - *Hint: No single distribution fits all scenarios equally well.*\n",
    "\n",
    "3. **How do Gringorten plotting positions help in flood frequency analysis?**\n",
    "   - *Hint: They're used to assign empirical probabilities to ordered data.*\n",
    "\n",
    "4. **What assumptions underlie the use of the Gumbel distribution in hydrology?**\n",
    "   - *Hint: It‚Äôs designed to model block maxima like annual peak flows.*\n",
    "\n",
    "5. **How do parametric and non-parametric flood frequency methods differ in their approach?**\n",
    "   - *Hint: Consider how the data distribution is treated.*\n",
    "\n",
    "---\n",
    "\n",
    "### Reflective Prompts\n",
    "\n",
    "1. **If two distributions yield similar RMSE but different KS p-values, which metric is more important for selecting a model‚Äîand why?**\n",
    "\n",
    "2. **Can a statistically good-fitting distribution be inappropriate for design applications? Provide an example.**\n",
    "\n",
    "3. **How would you adapt this tool to process data from multiple gage stations simultaneously?**\n",
    "\n",
    "4. **What limitations might this tool face when applied to future climate-affected streamflow patterns?**\n",
    "\n",
    "5. **How would the analysis change if you used partial-duration series instead of annual maxima?**\n",
    "\n",
    "---\n",
    "\n",
    "### Quiz Questions\n",
    "\n",
    "**Q1.** The Gumbel distribution is commonly used to model:  \n",
    "A. Rainfall intensity  \n",
    "B. Annual maximum values  \n",
    "C. Median flow durations  \n",
    "D. Baseflow during drought  \n",
    "‚úÖ **Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q2.** The Kolmogorov‚ÄìSmirnov test compares:  \n",
    "A. Log and normal distributions  \n",
    "B. ECDF and theoretical CDF  \n",
    "C. Mean annual rainfall  \n",
    "D. Number of peaks above threshold  \n",
    "‚úÖ **Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q3.** In the Generalized Extreme Value distribution, the shape parameter controls:  \n",
    "A. Peak discharge  \n",
    "B. Tail behavior  \n",
    "C. Cumulative runoff  \n",
    "D. Frequency of low flows  \n",
    "‚úÖ **Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q4.** A high KS p-value and low RMSE suggest:  \n",
    "A. Overfitting  \n",
    "B. Good model fit  \n",
    "C. Poor data resolution  \n",
    "D. Statistical bias  \n",
    "‚úÖ **Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q5.** Which distribution is least appropriate for positively skewed hydrologic data?  \n",
    "A. Gumbel  \n",
    "B. Lognormal  \n",
    "C. Normal  \n",
    "D. Log-Pearson III  \n",
    "‚úÖ **Correct:** C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d100d89-68e3-4c92-9cb3-a8f04eec3a5f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c77af9-fe6f-4f71-b14f-42aa3f27307c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0902ca5-7b10-46fb-9288-be0eb3c5f89c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
