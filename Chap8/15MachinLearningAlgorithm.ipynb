{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1665023-97b2-49f3-a902-62abb43a46f8",
   "metadata": {},
   "source": [
    "### ü§ñ Introduction to Machine Learning\n",
    "\n",
    "Machine learning (ML) is a field of artificial intelligence that enables systems to learn patterns from data and make predictions or decisions without being explicitly programmed.\n",
    "\n",
    "\n",
    "\n",
    "Machine Learning models represent foundational and advanced approaches used across supervised, unsupervised, and reinforcement learning tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ Neural Networks (NN)\n",
    "\n",
    "**Definition**: Composed of layers of interconnected nodes (neurons) that learn complex patterns through weighted connections.\n",
    "\n",
    "**Core Equation**:\n",
    "$$\n",
    "a^{(l)} = f(W^{(l)} a^{(l-1)} + b^{(l)})\n",
    "$$\n",
    "\n",
    "- \\( a^{(l)} \\): activation of layer \\( l \\)  \n",
    "- \\( W^{(l)} \\): weights  \n",
    "- \\( b^{(l)} \\): bias  \n",
    "- \\( f \\): activation function (e.g., ReLU, sigmoid)\n",
    "\n",
    "**Benefits**:\n",
    "- Learns nonlinear relationships  \n",
    "- Scalable to large datasets\n",
    "\n",
    "**Applicability**:\n",
    "- Classification, regression, time series prediction\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Reinforcement Neural Networks\n",
    "\n",
    "**Definition**: Neural networks used within reinforcement learning frameworks to approximate value functions or policies.\n",
    "\n",
    "**Example**: Deep Q-Network (DQN)\n",
    "\n",
    "**Q-Learning Update**:\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "**Benefits**:\n",
    "- Learns optimal strategies through trial and error  \n",
    "- Handles high-dimensional state spaces\n",
    "\n",
    "**Applicability**:\n",
    "- Robotics, game playing, autonomous control\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Convolutional Neural Networks (CNN)\n",
    "\n",
    "**Definition**: Specialized neural networks for processing grid-like data (e.g., images).\n",
    "\n",
    "**Core Operation**:\n",
    "$$\n",
    "Z_{i,j}^{(k)} = \\sum_{m,n} X_{i+m, j+n} \\cdot K_{m,n}^{(k)}\n",
    "$$\n",
    "\n",
    "- \\( X \\): input image  \n",
    "- \\( K \\): convolution kernel  \n",
    "- \\( Z \\): feature map\n",
    "\n",
    "**Benefits**:\n",
    "- Captures spatial hierarchies  \n",
    "- Reduces parameters via local connectivity\n",
    "\n",
    "**Applicability**:\n",
    "- Image classification, object detection, medical imaging\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Tree-Based Models\n",
    "\n",
    "#### üå≥ Decision Tree Regression\n",
    "\n",
    "**Definition**: Splits data recursively based on feature thresholds to predict continuous outcomes.\n",
    "\n",
    "**Prediction Rule**:\n",
    "- At each node, choose split that minimizes:\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y})^2\n",
    "$$\n",
    "\n",
    "**Benefits**:\n",
    "- Interpretable  \n",
    "- Handles nonlinear relationships\n",
    "\n",
    "**Applicability**:\n",
    "- Forecasting, risk modeling, feature importance analysis\n",
    "\n",
    "#### üå≤ Random Forest & Gradient Boosting\n",
    "\n",
    "- **Random Forest**: Ensemble of decision trees trained on bootstrapped samples  \n",
    "- **Gradient Boosting**: Sequentially builds trees to correct previous errors\n",
    "\n",
    "**Benefits**:\n",
    "- High accuracy  \n",
    "- Robust to overfitting (with tuning)\n",
    "\n",
    "**Applicability**:\n",
    "- Tabular data, structured prediction tasks\n",
    "\n",
    "---\n",
    "\n",
    "### 5Ô∏è‚É£ Other Popular Models\n",
    "\n",
    "| Model                  | Description                                   | Use Case Examples                        |\n",
    "|------------------------|-----------------------------------------------|------------------------------------------|\n",
    "| **Support Vector Machine (SVM)** | Finds optimal separating hyperplane         | Text classification, bioinformatics      |\n",
    "| **K-Nearest Neighbors (KNN)**    | Predicts based on closest training examples | Recommendation systems, anomaly detection |\n",
    "| **Naive Bayes**                 | Probabilistic model using Bayes‚Äô theorem    | Spam filtering, sentiment analysis       |\n",
    "| **Principal Component Analysis (PCA)** | Reduces dimensionality via orthogonal projection | Feature reduction, visualization     |\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Summary Table\n",
    "\n",
    "| Model Type                  | Strengths                          | Limitations                          | Best Use Cases                        |\n",
    "|-----------------------------|------------------------------------|--------------------------------------|--------------------------------------|\n",
    "| Neural Networks             | Nonlinear modeling, scalable       | Requires large data, less interpretable | General-purpose prediction           |\n",
    "| Reinforcement Neural Nets   | Strategic learning, adaptive       | Complex training, reward design      | Games, robotics                      |\n",
    "| CNN                         | Spatial feature extraction         | Requires structured input (e.g., images) | Vision tasks                         |\n",
    "| Tree Regression             | Interpretable, fast                | May overfit without pruning          | Tabular regression                   |\n",
    "| Random Forest / Boosting    | High accuracy, ensemble power      | Slower training, tuning needed       | Structured data, classification      |\n",
    "| SVM                         | Effective in high dimensions       | Sensitive to kernel choice           | Text, image classification           |\n",
    "| KNN                         | Simple, intuitive                  | Slow for large datasets              | Recommendation, anomaly detection    |\n",
    "| Naive Bayes                 | Fast, probabilistic                | Assumes feature independence         | Text, spam filtering                 |\n",
    "| PCA                         | Dimensionality reduction           | May lose interpretability            | Preprocessing, visualization         |\n",
    "\n",
    "---\n",
    "\n",
    "### üìÇ Datasets in Machine Learning\n",
    "\n",
    "- **Training Dataset**: Used to teach the model by adjusting internal parameters based on input-output pairs.\n",
    "- **Validation Dataset**: Used to tune model hyperparameters and prevent overfitting during training.\n",
    "- **Test Dataset**: Used to evaluate final model performance on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Types of Machine Learning Algorithms\n",
    "\n",
    "\n",
    "Machine learning algorithms are categorized based on how they learn from data. Below are the four primary types:\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ Supervised Learning\n",
    "\n",
    "**Definition**: Learns from labeled data ‚Äî each input is paired with a known output.\n",
    "\n",
    "**Examples**:\n",
    "- Linear Regression\n",
    "- Decision Trees\n",
    "- Support Vector Machines (SVM)\n",
    "- Neural Networks\n",
    "\n",
    "**Equation (Regression Example)**:\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + \\epsilon\n",
    "$$\n",
    "\n",
    "**Benefits**:\n",
    "- High accuracy\n",
    "- Interpretable models\n",
    "- Predictive power\n",
    "\n",
    "**Applicability**:\n",
    "- Spam detection, credit scoring, medical diagnosis\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Unsupervised Learning\n",
    "\n",
    "**Definition**: Finds patterns or structure in unlabeled data.\n",
    "\n",
    "**Examples**:\n",
    "- K-Means Clustering\n",
    "- Principal Component Analysis (PCA)\n",
    "- Hierarchical Clustering\n",
    "\n",
    "**Equation (K-Means Objective)**:\n",
    "$$\n",
    "\\min \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\| x - \\mu_i \\|^2\n",
    "$$\n",
    "\n",
    "**Benefits**:\n",
    "- Useful for exploratory analysis\n",
    "- Reveals hidden structures\n",
    "\n",
    "**Applicability**:\n",
    "- Customer segmentation, anomaly detection, dimensionality reduction\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Semi-Supervised Learning\n",
    "\n",
    "**Definition**: Combines a small amount of labeled data with a large amount of unlabeled data.\n",
    "\n",
    "**Examples**:\n",
    "- Self-training classifiers\n",
    "- Graph-based label propagation\n",
    "\n",
    "**Benefits**:\n",
    "- Reduces labeling cost\n",
    "- Improves generalization\n",
    "\n",
    "**Applicability**:\n",
    "- Web content classification, speech recognition, bioinformatics\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Reinforcement Learning\n",
    "\n",
    "**Definition**: Learns by interacting with an environment and receiving feedback (rewards or penalties).\n",
    "\n",
    "**Examples**:\n",
    "- Q-Learning\n",
    "- Deep Q Networks (DQN)\n",
    "- Policy Gradient Methods\n",
    "\n",
    "**Core Equation (Q-Learning Update)**:\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "- \\( Q(s, a) \\): expected reward for action \\( a \\) in state \\( s \\)  \n",
    "- \\( \\alpha \\): learning rate  \n",
    "- \\( \\gamma \\): discount factor  \n",
    "- \\( r \\): reward  \n",
    "- \\( s' \\): next state\n",
    "\n",
    "**Benefits**:\n",
    "- Effective in dynamic environments\n",
    "- Learns optimal strategies\n",
    "\n",
    "**Applicability**:\n",
    "- Robotics, game playing, autonomous systems\n",
    "\n",
    "---\n",
    "### üìä Summary Table: Machine Learning Algorithm Types\n",
    "\n",
    "| Type                  | Description                                | Strengths                                           | Weaknesses                                      |\n",
    "|-----------------------|--------------------------------------------|----------------------------------------------------|-------------------------------------------------|\n",
    "| Supervised Learning   | Learns from labeled data                   | High accuracy, interpretable models                | Requires large labeled datasets                 |\n",
    "| Unsupervised Learning | Finds structure in unlabeled data          | Useful for exploratory analysis                    | Hard to evaluate performance                    |\n",
    "| Semi-Supervised       | Mix of labeled and unlabeled data          | Reduces labeling cost                              | Still needs some labeled data                  |\n",
    "| Reinforcement Learning| Learns via trial and error                 | Effective in dynamic environments                  | Complex, requires extensive training time       |\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è Training Algorithms\n",
    "\n",
    "Common training algorithms include:\n",
    "- **Gradient Descent**: Optimizes model parameters by minimizing a loss function.\n",
    "- **Backpropagation**: Used in neural networks to update weights via gradients.\n",
    "- **Expectation-Maximization**: Used in probabilistic models like Gaussian Mixture Models.\n",
    "- **Evolutionary Algorithms**: Inspired by natural selection, used for optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### 1Ô∏è‚É£ Gradient Descent\n",
    "\n",
    "**Purpose**: Minimize a loss function by iteratively updating model parameters.\n",
    "\n",
    "**Update Rule**:\n",
    "$$\n",
    "\\theta := \\theta - \\alpha \\cdot \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "- $\\theta$: model parameters  \n",
    "- $\\alpha$: learning rate  \n",
    "- $J(\\theta)$: loss function  \n",
    "- $\\nabla_\\theta J(\\theta)$: gradient of the loss with respect to parameters\n",
    "\n",
    "**Benefits**:\n",
    "- Simple and widely applicable\n",
    "- Scalable to large datasets (with variants like stochastic and mini-batch)\n",
    "\n",
    "**Applicability**:\n",
    "- Linear regression, logistic regression, neural networks, SVMs\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Backpropagation\n",
    "\n",
    "**Purpose**: Efficiently compute gradients in neural networks using the chain rule.\n",
    "\n",
    "**Core Equation**:\n",
    "For each layer \\( l \\), the error term:\n",
    "$$\n",
    "\\delta^l = (W^{l+1})^T \\delta^{l+1} \\circ f'(z^l)\n",
    "$$\n",
    "\n",
    "- $\\delta^l$: error at layer \\( l \\)  \n",
    "- $W^{l+1}$: weights of next layer  \n",
    "- $f'(z^l)$: derivative of activation function  \n",
    "- $\\circ$: element-wise multiplication\n",
    "\n",
    "**Benefits**:\n",
    "- Enables deep learning by propagating error backward\n",
    "- Efficient gradient computation for complex architectures\n",
    "\n",
    "**Applicability**:\n",
    "- Deep neural networks, CNNs, RNNs\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Expectation-Maximization (EM)\n",
    "\n",
    "**Purpose**: Estimate parameters in probabilistic models with latent variables.\n",
    "\n",
    "**Steps**:\n",
    "- **E-step**: Estimate expected value of latent variables given current parameters  \n",
    "- **M-step**: Maximize likelihood with respect to parameters\n",
    "\n",
    "**Equations**:\n",
    "E-step:\n",
    "$$\n",
    "Q(\\theta | \\theta^{(t)}) = \\mathbb{E}_{Z | X, \\theta^{(t)}}[\\log p(X, Z | \\theta)]\n",
    "$$\n",
    "\n",
    "M-step:\n",
    "$$\n",
    "\\theta^{(t+1)} = \\arg\\max_\\theta Q(\\theta | \\theta^{(t)})\n",
    "$$\n",
    "\n",
    "**Benefits**:\n",
    "- Handles missing or hidden data\n",
    "- Converges to local optima\n",
    "\n",
    "**Applicability**:\n",
    "- Gaussian Mixture Models (GMM), Hidden Markov Models (HMM), clustering\n",
    "\n",
    "---\n",
    "\n",
    "### 4Ô∏è‚É£ Evolutionary Algorithms\n",
    "\n",
    "**Purpose**: Optimize solutions using principles of natural selection.\n",
    "\n",
    "**Steps**:\n",
    "1. Initialize population of candidate solutions  \n",
    "2. Evaluate fitness  \n",
    "3. Select, crossover, and mutate  \n",
    "4. Repeat until convergence\n",
    "\n",
    "**Key Concepts**:\n",
    "- **Fitness function**: evaluates solution quality  \n",
    "- **Mutation**: introduces randomness  \n",
    "- **Crossover**: combines solutions\n",
    "\n",
    "**Benefits**:\n",
    "- Global search capability\n",
    "- No gradient required\n",
    "\n",
    "**Applicability**:\n",
    "- Optimization problems, neural architecture search, game strategies\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Summary Table\n",
    "\n",
    "| Algorithm               | Equation Type                     | Benefits                          | Applicability                         |\n",
    "|------------------------|-----------------------------------|-----------------------------------|--------------------------------------|\n",
    "| Gradient Descent       | Gradient-based update             | Fast, scalable                    | Regression, classification           |\n",
    "| Backpropagation        | Chain rule for gradients          | Enables deep learning             | Neural networks                      |\n",
    "| Expectation-Maximization | Probabilistic expectation-maximization | Handles latent variables         | GMM, HMM, clustering                 |\n",
    "| Evolutionary Algorithms| Population-based search           | Global optimization, flexible     | Optimization, strategy search        |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7049822-7043-43a1-86b5-df5c916f5f54",
   "metadata": {},
   "source": [
    "### üß† Interactive K-Means Clustering Explorer\n",
    "\n",
    "This module introduces students to the core concepts of unsupervised learning through K-Means clustering.\n",
    "\n",
    "### üì¶ Core Components\n",
    "- `make_blobs`: Generates synthetic data with user-defined clusters\n",
    "- `KMeans`: Applies clustering algorithm to assign labels and compute centroids\n",
    "- `matplotlib`: Visualizes clustered data and centroids\n",
    "- `ipywidgets`: Enables interactive control of clustering parameters\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "- Understand how K-Means partitions data into clusters based on proximity\n",
    "- Explore the impact of:\n",
    "  - Number of clusters (`k`)\n",
    "  - Cluster spread (`std`)\n",
    "  - Sample size (`n`)\n",
    "- Visualize centroids and cluster assignments\n",
    "- Interpret inertia (sum of squared distances to centroids) as a measure of fit\n",
    "\n",
    "### üîç Interactive Controls\n",
    "- `Clusters (k)`: Number of clusters to form\n",
    "- `Cluster Std`: Standard deviation of each cluster\n",
    "- `Samples`: Total number of data points\n",
    "\n",
    "### üìä Output Summary\n",
    "- Scatter plot of clustered data with colored labels\n",
    "- Red \"X\" markers for cluster centroids\n",
    "- Markdown summary of parameters and inertia\n",
    "\n",
    "### üß™ Extension Ideas\n",
    "- Add elbow method visualization to choose optimal `k`\n",
    "- Compare with hierarchical or DBSCAN clustering\n",
    "- Apply to real-world datasets (e.g., Iris, customer segmentation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b795a8af-3c48-4c15-8286-131b6a6d4230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üß† Interactive K-Means Clustering Explorer"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "793c52235fa2487a804f97e3a26187cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=3, description='Clusters (k)', max=10, min=1), FloatSlider(value=1.0, descripti‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372f5f9027374859b767f29e791b3b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üì¶ Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "# üéØ Synthetic data generator\n",
    "def generate_data(n_samples=300, n_features=2, centers=4, cluster_std=1.0):\n",
    "    X, _ = make_blobs(n_samples=n_samples, centers=centers, n_features=n_features,\n",
    "                      cluster_std=cluster_std, random_state=42)\n",
    "    return X\n",
    "\n",
    "# üîç Interactive clustering\n",
    "def explore_kmeans(n_clusters=3, std=1.0, samples=300):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Generate data\n",
    "    X = generate_data(n_samples=samples, centers=n_clusters, cluster_std=std)\n",
    "    \n",
    "    # Fit KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', alpha=0.6, edgecolor='k')\n",
    "    plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X', label='Centroids')\n",
    "    plt.title(f\"K-Means Clustering (k={n_clusters})\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Display summary\n",
    "    display(Markdown(f\"\"\"\n",
    "### üìä K-Means Summary\n",
    "- Number of clusters: **{n_clusters}**\n",
    "- Samples: **{samples}**\n",
    "- Cluster standard deviation: **{std}**\n",
    "- Inertia (sum of squared distances to centroids): **{kmeans.inertia_:.2f}**\n",
    "\"\"\"))\n",
    "\n",
    "# üéõÔ∏è Widgets\n",
    "cluster_slider = widgets.IntSlider(value=3, min=1, max=10, step=1, description='Clusters (k)')\n",
    "std_slider = widgets.FloatSlider(value=1.0, min=0.1, max=2.0, step=0.1, description='Cluster Std')\n",
    "sample_slider = widgets.IntSlider(value=300, min=100, max=1000, step=50, description='Samples')\n",
    "\n",
    "# ‚ñ∂Ô∏è Display\n",
    "ui = widgets.VBox([cluster_slider, std_slider, sample_slider])\n",
    "out = widgets.interactive_output(explore_kmeans, {\n",
    "    'n_clusters': cluster_slider,\n",
    "    'std': std_slider,\n",
    "    'samples': sample_slider\n",
    "})\n",
    "\n",
    "display(Markdown(\"## üß† Interactive K-Means Clustering Explorer\"))\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8389c2-20f3-4466-a2a1-41412e5e1103",
   "metadata": {},
   "source": [
    "### üß† Regression Model Comparison: Polynomial, Tree, and Neural Network\n",
    "\n",
    "This module demonstrates how different regression algorithms learn from data and generalize to unseen inputs. It includes:\n",
    "\n",
    "### üì¶ Core Components\n",
    "- `numpy`, `matplotlib`: Data generation and visualization\n",
    "- `sklearn`: Models and metrics\n",
    "  - `PolynomialFeatures` + `LinearRegression`\n",
    "  - `DecisionTreeRegressor`\n",
    "  - `MLPRegressor` (Neural Network)\n",
    "- `train_test_split`: Calibration/validation data separation\n",
    "- `ipywidgets`: Interactive parameter control\n",
    "- `pandas`: Tabular performance summary\n",
    "\n",
    "### üéØ Workflow Overview\n",
    "1. **Synthetic Data Generation**  \n",
    "   - Cubic function with noise: \\( y = 0.5x^3 - x^2 + x + \\varepsilon \\)\n",
    "\n",
    "2. **User-Controlled Parameters**\n",
    "   - Polynomial degree  \n",
    "   - Tree depth  \n",
    "   - Neural network architecture (2 hidden layers + activation)  \n",
    "   - Calibration/validation split ratio\n",
    "\n",
    "3. **Model Training & Prediction**\n",
    "   - Fit each model on calibration data  \n",
    "   - Predict on both calibration and validation sets\n",
    "\n",
    "4. **Visualization**\n",
    "   - Overlay predictions on validation data  \n",
    "   - Compare model shapes and smoothness\n",
    "\n",
    "5. **Performance Summary**\n",
    "   - Mean Squared Error (MSE) for each model on both datasets  \n",
    "   - Polynomial equation display  \n",
    "   - Tree and neural network structure summary\n",
    "\n",
    "### üìä Learning Objectives\n",
    "- Understand how model complexity affects fit and generalization  \n",
    "- Explore bias-variance tradeoff across algorithms  \n",
    "- Compare smooth vs piecewise vs flexible learning strategies  \n",
    "- Interpret calibration vs validation performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71980151-4004-4cf4-9325-7b6da0c6f07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## üîç Regression Explorer with Calibration/Validation Split"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b0a09af10041debcef85210d020713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntSlider(value=3, description='Poly Degree', max=10, min=1), IntSlider(value=3, description='T‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d53e3b5804a4329b14245bc1dd4741f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üì¶ Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import pandas as pd\n",
    "\n",
    "# üéØ Synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 100).reshape(-1, 1)\n",
    "y = 0.5 * X**3 - X**2 + X + np.random.normal(0, 1, size=X.shape)\n",
    "\n",
    "# üîç Interactive comparison\n",
    "def compare_models(degree=3, tree_depth=3, hidden1=10, hidden2=10, activation='relu', split_ratio=0.7):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Split data\n",
    "    X_cal, X_val, y_cal, y_val = train_test_split(X, y, train_size=split_ratio, random_state=42)\n",
    "    \n",
    "    # Polynomial Regression\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_cal_poly = poly.fit_transform(X_cal)\n",
    "    X_val_poly = poly.transform(X_val)\n",
    "    poly_model = LinearRegression()\n",
    "    poly_model.fit(X_cal_poly, y_cal)\n",
    "    y_poly_cal = poly_model.predict(X_cal_poly)\n",
    "    y_poly_val = poly_model.predict(X_val_poly)\n",
    "    \n",
    "    # Extract polynomial equation\n",
    "    coefs = poly_model.coef_.flatten()\n",
    "    intercept = poly_model.intercept_\n",
    "    terms = [f\"{intercept[0]:.2f}\"]\n",
    "    for i, c in enumerate(coefs[1:], start=1):\n",
    "        terms.append(f\"{c:.2f}¬∑x^{i}\")\n",
    "    equation = \" + \".join(terms)\n",
    "    \n",
    "    # Decision Tree Regression\n",
    "    tree_model = DecisionTreeRegressor(max_depth=tree_depth, random_state=42)\n",
    "    tree_model.fit(X_cal, y_cal)\n",
    "    y_tree_cal = tree_model.predict(X_cal)\n",
    "    y_tree_val = tree_model.predict(X_val)\n",
    "    \n",
    "    # Neural Network Regression\n",
    "    nn_model = MLPRegressor(hidden_layer_sizes=(hidden1, hidden2), activation=activation,\n",
    "                            max_iter=1000, random_state=42)\n",
    "    nn_model.fit(X_cal, y_cal.ravel())\n",
    "    y_nn_cal = nn_model.predict(X_cal)\n",
    "    y_nn_val = nn_model.predict(X_val)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X_val, y_val, color='gray', alpha=0.5, label='Validation Data')\n",
    "    plt.plot(np.sort(X_val, axis=0), y_poly_val[np.argsort(X_val.ravel())], label='Polynomial', color='blue')\n",
    "    plt.plot(np.sort(X_val, axis=0), y_tree_val[np.argsort(X_val.ravel())], label='Tree', color='green')\n",
    "    plt.plot(np.sort(X_val, axis=0), y_nn_val[np.argsort(X_val.ravel())], label='Neural Net', color='red')\n",
    "    plt.title(\"Model Comparison on Validation Data\")\n",
    "    plt.xlabel(\"X\")\n",
    "    plt.ylabel(\"Predicted y\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Metrics Table\n",
    "    metrics = pd.DataFrame({\n",
    "        \"Model\": [\"Polynomial\", \"Decision Tree\", \"Neural Network\"],\n",
    "        \"Calibration MSE\": [\n",
    "            mean_squared_error(y_cal, y_poly_cal),\n",
    "            mean_squared_error(y_cal, y_tree_cal),\n",
    "            mean_squared_error(y_cal, y_nn_cal)\n",
    "        ],\n",
    "        \"Validation MSE\": [\n",
    "            mean_squared_error(y_val, y_poly_val),\n",
    "            mean_squared_error(y_val, y_tree_val),\n",
    "            mean_squared_error(y_val, y_nn_val)\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    display(Markdown(f\"### üìò Polynomial Equation (degree {degree})\\n**y =** {equation}\"))\n",
    "    display(Markdown(\"### üìä Calibration vs Validation Performance\"))\n",
    "    display(metrics.style.format({\n",
    "        \"Calibration MSE\": \"{:.4f}\",\n",
    "        \"Validation MSE\": \"{:.4f}\"\n",
    "    }).set_caption(\"Mean Squared Error (MSE)\"))\n",
    "    \n",
    "    display(Markdown(f\"\"\"\n",
    "### üå≥ Tree Structure\n",
    "- Max Depth: `{tree_depth}`\n",
    "\n",
    "### üß† Neural Network Structure\n",
    "- Hidden Layers: ({hidden1}, {hidden2})\n",
    "- Activation Function: `{activation}`\n",
    "\"\"\"))\n",
    "\n",
    "# üéõÔ∏è Widgets\n",
    "degree_slider = widgets.IntSlider(value=3, min=1, max=10, step=1, description='Poly Degree')\n",
    "tree_depth_slider = widgets.IntSlider(value=3, min=1, max=10, step=1, description='Tree Depth')\n",
    "hidden1_slider = widgets.IntSlider(value=10, min=1, max=100, step=1, description='Hidden Layer 1')\n",
    "hidden2_slider = widgets.IntSlider(value=10, min=1, max=100, step=1, description='Hidden Layer 2')\n",
    "activation_dropdown = widgets.Dropdown(options=['relu', 'tanh', 'logistic'], value='relu', description='Activation')\n",
    "split_slider = widgets.FloatSlider(value=0.7, min=0.5, max=0.9, step=0.05, description='Train Split')\n",
    "\n",
    "# ‚ñ∂Ô∏è Display\n",
    "ui = widgets.VBox([\n",
    "    degree_slider, tree_depth_slider, hidden1_slider, hidden2_slider,\n",
    "    activation_dropdown, split_slider\n",
    "])\n",
    "out = widgets.interactive_output(compare_models, {\n",
    "    'degree': degree_slider,\n",
    "    'tree_depth': tree_depth_slider,\n",
    "    'hidden1': hidden1_slider,\n",
    "    'hidden2': hidden2_slider,\n",
    "    'activation': activation_dropdown,\n",
    "    'split_ratio': split_slider\n",
    "})\n",
    "\n",
    "display(Markdown(\"### üîç Regression Explorer with Calibration/Validation Split\"))\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28361f7-ed36-4e10-af20-99b7fab624c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08033509-88d7-4f3b-9912-35ae362dcb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
