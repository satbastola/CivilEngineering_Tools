{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe0a4394-f393-483f-be77-57092dcc23f7",
   "metadata": {},
   "source": [
    "# Chapter 2 Hydrology: Flood Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf62ca6-9c4f-4646-842b-d3d6a915b963",
   "metadata": {},
   "source": [
    "1. [Introduction: Flood frequency](#1.-Introduction)\n",
    "2. [Simulation: EVD performance](#2.-Simulation)\n",
    "3. [Self-Assessment](#3.-Self-Assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9ceb65-e216-441e-a38d-13e59bc17273",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec99960-92ba-4cf7-b9b2-c81f39b45b25",
   "metadata": {},
   "source": [
    "### Flood frequency analysis tool\n",
    "\n",
    "This tool reads peak flow data from the USGS NWIS database and fits 10 commonly used extreme value probability distributions to estimate flood magnitudes associated with various return periods (e.g., 2-year, 100-year). It performs statistical goodness-of-fit evaluation and provides an interactive interface to visualize the flood frequency curve for each distribution.\n",
    "\n",
    "---\n",
    "\n",
    "###  What the Tool Does\n",
    "\n",
    "- ✅ Reads annual peak discharge data from a NWIS `.txt` file\n",
    "- ✅ Fits multiple statistical distributions to the observed peak flows\n",
    "- ✅ Computes estimated flood quantiles for specific return periods (2, 5, 10, 25, 50, 100 years)\n",
    "- ✅ Calculates RMSE and Kolmogorov–Smirnov (KS) goodness-of-fit metrics\n",
    "- ✅ Allows the user to interactively select a distribution and view:\n",
    "  - Estimated peak flows\n",
    "  - Distribution parameters\n",
    "  - GOF statistics\n",
    "  - A flood frequency curve plotted in log scale\n",
    "\n",
    "---\n",
    "\n",
    "###  How to Use\n",
    "\n",
    "1. **Prepare Input File**  \n",
    "   - Download annual peak streamflow data from the [USGS NWIS Peak Flow site](https://waterdata.usgs.gov/nwis/peak)\n",
    "   - Save as a tab-delimited `.txt` file (e.g., `07022500_nwis_peak.txt`)\n",
    "\n",
    "2. **Run the Script in Jupyter Notebook**\n",
    "   - Place the file in your working directory\n",
    "   - Modify the line `usgs_file = \"07022500_nwis_peak.txt\"` to match your filename\n",
    "   - Run the script cell-by-cell\n",
    "\n",
    "3. **Explore Results**\n",
    "   - View the summary table of fitted distribution parameters and their statistical performance\n",
    "   - Use the dropdown selector to compare estimated flood flows and curves for each distribution\n",
    "\n",
    "---\n",
    "\n",
    "### Theoretical Background: Distributions Used\n",
    "\n",
    "Each distribution estimates the probability of rare flood events based on historical data. Here's a quick reference:\n",
    "\n",
    "| Distribution           | Description                                                                 | Parameters                        |\n",
    "|------------------------|-----------------------------------------------------------------------------|-----------------------------------|\n",
    "| **Gumbel (EV1)**        | Models block maxima (e.g., annual max). Skewed right.                      | Location (μ), Scale (β)           |\n",
    "| **Log-Pearson III**     | Log-transformed Pearson Type III. Used in U.S. federal flood studies.      | Shape (α), Location (μ), Scale    |\n",
    "| **GEV**                 | General form for extremes. Includes Gumbel, Frechet, Weibull as cases.     | Shape (ξ), Location, Scale        |\n",
    "| **Normal**              | Symmetric bell curve. May misrepresent skewed flood data.                  | Mean (μ), Std. dev. (σ)           |\n",
    "| **Lognormal**           | Data is normally distributed after log transform. Skewed right.            | Shape (σ), Location, Scale        |\n",
    "| **Weibull (Type III)**  | Useful for extreme minimums or upper tails.                                | Shape (k), Location, Scale        |\n",
    "| **Exponential**         | Special case of Weibull; constant failure rate (rarely used for floods).   | Rate (λ) or Scale                 |\n",
    "| **Gamma**               | General skewed distribution, flexible fit for hydrology                    | Shape (k), Scale (θ), Location    |\n",
    "| **Loglogistic (Fisk)**  | Skewed right, like lognormal but heavier tail.                             | Shape (c), Location, Scale        |\n",
    "| **Generalized Pareto**  | Models excesses over a threshold (POT approach).                           | Shape, Location, Scale            |\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Evaluation Criteria\n",
    "\n",
    "Two statistical metrics assess how well each distribution fits the observed data:\n",
    "\n",
    "- ### 🔹 Root Mean Squared Error (RMSE)\n",
    "  Measures average error between observed peak flows and estimated quantiles from the distribution:\n",
    "  $$\n",
    "  \\text{RMSE} = \\sqrt{ \\frac{1}{n} \\sum (Q_{\\text{obs}} - Q_{\\text{est}})^2 }\n",
    "  $$\n",
    "  Lower values indicate a better fit.\n",
    "\n",
    "- ### 🔹 Kolmogorov–Smirnov (KS) Statistic\n",
    "  Measures the maximum difference between the empirical cumulative distribution function (ECDF) and the theoretical CDF:\n",
    "  $$\n",
    "  D = \\sup_x |F_n(x) - F(x)|\n",
    "  $$\n",
    "  - Returns both the **KS statistic** and a **p-value**\n",
    "  - If p-value > 0.05: distribution is a statistically valid fit (✅ Pass)\n",
    "\n",
    "---\n",
    "\n",
    "### Output Summary\n",
    "\n",
    "- A sorted summary table of all distributions including:\n",
    "  - Fitted parameters\n",
    "  - RMSE\n",
    "  - KS statistic and p-value\n",
    "  - Pass/fail interpretation\n",
    "- Interactive flood frequency plots for return periods on a log-x axis\n",
    "- Ability to choose which distribution best represents the dataset\n",
    "\n",
    "---\n",
    "\n",
    "### Applications\n",
    "\n",
    "- Floodplain mapping\n",
    "- Hydraulic structure design (culverts, bridges, dams)\n",
    "- Return period–based risk estimation\n",
    "- Hydrologic modeling calibration\n",
    "\n",
    "---\n",
    "\n",
    "Let me know if you'd like this tool extended with confidence intervals, percentile shading, or exported reports in Excel or PDF!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c87f8-1730-4016-8b01-f7ea2fa1e40c",
   "metadata": {},
   "source": [
    "## 2. Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00cb1698-bce1-420c-bccd-c57addd2bc86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency_cd</th>\n",
       "      <th>site_no</th>\n",
       "      <th>peak_dt</th>\n",
       "      <th>peak_va</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USGS</td>\n",
       "      <td>7022500</td>\n",
       "      <td>3/3/1953</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USGS</td>\n",
       "      <td>7022500</td>\n",
       "      <td>1/20/1954</td>\n",
       "      <td>520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USGS</td>\n",
       "      <td>7022500</td>\n",
       "      <td>3/20/1955</td>\n",
       "      <td>846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USGS</td>\n",
       "      <td>7022500</td>\n",
       "      <td>2/2/1956</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USGS</td>\n",
       "      <td>7022500</td>\n",
       "      <td>1/22/1957</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  agency_cd  site_no    peak_dt  peak_va\n",
       "0      USGS  7022500   3/3/1953      780\n",
       "1      USGS  7022500  1/20/1954      520\n",
       "2      USGS  7022500  3/20/1955      846\n",
       "3      USGS  7022500   2/2/1956      440\n",
       "4      USGS  7022500  1/22/1957      707"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      780\n",
      "1      520\n",
      "2      846\n",
      "3      440\n",
      "4      707\n",
      "5      763\n",
      "6      514\n",
      "7      602\n",
      "8      304\n",
      "9      833\n",
      "10     228\n",
      "11     690\n",
      "12    1870\n",
      "13     545\n",
      "14     702\n",
      "15     295\n",
      "16     987\n",
      "17     350\n",
      "18     816\n",
      "19     900\n",
      "20    1160\n",
      "21     800\n",
      "22     748\n",
      "23     728\n",
      "24    1460\n",
      "25     364\n",
      "26     975\n",
      "27    1080\n",
      "28    2760\n",
      "29     954\n",
      "30     660\n",
      "31     360\n",
      "32     415\n",
      "Name: peak_va, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Simulate the file content as a string\n",
    "data = \"\"\"\\\n",
    "agency_cd   site_no peak_dt peak_va\n",
    "USGS    7022500 3/3/1953    780\n",
    "USGS    7022500 1/20/1954   520\n",
    "USGS    7022500 3/20/1955   846\n",
    "USGS    7022500 2/2/1956    440\n",
    "USGS    7022500 1/22/1957   707\n",
    "USGS    7022500 11/18/1957  763\n",
    "USGS    7022500 8/6/1959    514\n",
    "USGS    7022500 7/3/1960    602\n",
    "USGS    7022500 3/12/1961   304\n",
    "USGS    7022500 9/16/1962   833\n",
    "USGS    7022500 3/4/1963    228\n",
    "USGS    7022500 3/4/1964    690\n",
    "USGS    7022500 3/29/1965   1870\n",
    "USGS    7022500 3/20/1968   545\n",
    "USGS    7022500 4/9/1969    702\n",
    "USGS    7022500 6/13/1970   295\n",
    "USGS    7022500 8/21/1971   987\n",
    "USGS    7022500 7/28/1972   350\n",
    "USGS    7022500 5/1/1973    816\n",
    "USGS    7022500 11/24/1973  900\n",
    "USGS    7022500 3/29/1975   1160\n",
    "USGS    7022500 2/17/1976   800\n",
    "USGS    7022500 6/26/1977   748\n",
    "USGS    7022500 3/14/1978   728\n",
    "USGS    7022500 12/3/1978   1460\n",
    "USGS    7022500 3/17/1980   364\n",
    "USGS    7022500 6/6/1981    975\n",
    "USGS    7022500 1/4/1982    1080\n",
    "USGS    7022500 6/3/1983    2760\n",
    "USGS    7022500 4/29/1984   954\n",
    "USGS    7022500 9/5/1985    660\n",
    "USGS    7022500 5/24/1986   360\n",
    "USGS    7022500 2/28/1987   415\n",
    "\"\"\"\n",
    "\n",
    "# Read it as a DataFrame using tab separator\n",
    "peak_df = pd.read_csv(StringIO(data), sep=\"\\t\")\n",
    "#df = pd.read_csv(StringIO(data), delim_whitespace=True)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "display(df.head())\n",
    "# Display the first few rows\n",
    "#df.head()\n",
    "\n",
    "print(df['peak_va'])\n",
    "\n",
    "#peak_df[\"peak_va\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7643db02-d399-40cf-99b6-696997476e8e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Goodness-of-Fit Summary for All Distributions:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Distribution</th>\n",
       "      <th>Parameters</th>\n",
       "      <th>RMSE (cfs)</th>\n",
       "      <th>KS Stat</th>\n",
       "      <th>KS p-value</th>\n",
       "      <th>KS Result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Loglogistic</td>\n",
       "      <td>3.00, 70.55, 612.21</td>\n",
       "      <td>64.90</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.734</td>\n",
       "      <td>✅ Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GEV</td>\n",
       "      <td>-0.18, 571.63, 277.22</td>\n",
       "      <td>72.37</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.671</td>\n",
       "      <td>✅ Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lognormal</td>\n",
       "      <td>0.60, 71.09, 602.92</td>\n",
       "      <td>86.59</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.650</td>\n",
       "      <td>✅ Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Log-Pearson III</td>\n",
       "      <td>1.56, 792.61, 456.75</td>\n",
       "      <td>114.93</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.710</td>\n",
       "      <td>✅ Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gamma</td>\n",
       "      <td>1.64, 208.17, 356.96</td>\n",
       "      <td>114.93</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.710</td>\n",
       "      <td>✅ Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Weibull</td>\n",
       "      <td>0.86, 350.00, 439.66</td>\n",
       "      <td>129.30</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.334</td>\n",
       "      <td>✅ Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Exponential</td>\n",
       "      <td>228.00, 564.61</td>\n",
       "      <td>139.19</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.258</td>\n",
       "      <td>✅ Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Generalized Pareto</td>\n",
       "      <td>-0.17, 219.25, 671.26</td>\n",
       "      <td>144.60</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.515</td>\n",
       "      <td>✅ Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gumbel (EV1)</td>\n",
       "      <td>601.78, 303.40</td>\n",
       "      <td>153.94</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.910</td>\n",
       "      <td>✅ Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Normal</td>\n",
       "      <td>792.61, 484.53</td>\n",
       "      <td>234.72</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.151</td>\n",
       "      <td>✅ Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Distribution             Parameters  RMSE (cfs)  KS Stat  KS p-value  \\\n",
       "8         Loglogistic    3.00, 70.55, 612.21       64.90    0.115       0.734   \n",
       "2                 GEV  -0.18, 571.63, 277.22       72.37    0.121       0.671   \n",
       "4           Lognormal    0.60, 71.09, 602.92       86.59    0.124       0.650   \n",
       "1     Log-Pearson III   1.56, 792.61, 456.75      114.93    0.117       0.710   \n",
       "7               Gamma   1.64, 208.17, 356.96      114.93    0.117       0.710   \n",
       "5             Weibull   0.86, 350.00, 439.66      129.30    0.160       0.334   \n",
       "6         Exponential         228.00, 564.61      139.19    0.171       0.258   \n",
       "9  Generalized Pareto  -0.17, 219.25, 671.26      144.60    0.138       0.515   \n",
       "0        Gumbel (EV1)         601.78, 303.40      153.94    0.093       0.910   \n",
       "3              Normal         792.61, 484.53      234.72    0.193       0.151   \n",
       "\n",
       "  KS Result  \n",
       "8    ✅ Pass  \n",
       "2    ✅ Pass  \n",
       "4    ✅ Pass  \n",
       "1    ✅ Pass  \n",
       "7    ✅ Pass  \n",
       "5    ✅ Pass  \n",
       "6    ✅ Pass  \n",
       "9    ✅ Pass  \n",
       "0    ✅ Pass  \n",
       "3    ✅ Pass  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33278aaba1c745858b7a8a414f7dbb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Distribution', options=('Gumbel (EV1)', 'Log-Pearson III', 'GEV', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_selected_distribution(dist_name)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Required Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import kstest\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from ipywidgets import interact, Dropdown\n",
    "from IPython.display import display\n",
    "#pip install scipy\n",
    "### Load NWIS Peak Flow Data\n",
    "# def read_nwis_peak_file(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, 'r') as f:\n",
    "#             lines = f.readlines()\n",
    "#         start_line = next(i for i, line in enumerate(lines) if not line.startswith('#'))\n",
    "#         df = pd.read_csv(\n",
    "#             file_path,\n",
    "#             sep='\\t',\n",
    "#             comment='#',\n",
    "#             header=0,\n",
    "#             dtype=str,\n",
    "#             engine='python'\n",
    "#         )\n",
    "#         df.columns = df.columns.str.strip()\n",
    "#         df['peak_dt'] = pd.to_datetime(df['peak_dt'], errors='coerce')\n",
    "#         df['peak_va'] = pd.to_numeric(df['peak_va'], errors='coerce')\n",
    "#         df_clean = df[['site_no', 'peak_dt', 'peak_va']].dropna()\n",
    "#         return df_clean\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error reading file: {e}\")\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "### Load & Preview Data\n",
    "# usgs_file = \"07022500_nwis_peak.txt\"\n",
    "# peak_df = read_nwis_peak_file(usgs_file)\n",
    "# display(peak_df.head())\n",
    "\n",
    "### Set Up Distribution Parameters\n",
    "distributions = {\n",
    "    \"Gumbel (EV1)\": stats.gumbel_r,\n",
    "    \"Log-Pearson III\": stats.pearson3,\n",
    "    \"GEV\": stats.genextreme,\n",
    "    \"Normal\": stats.norm,\n",
    "    \"Lognormal\": stats.lognorm,\n",
    "    \"Weibull\": stats.weibull_min,\n",
    "    \"Exponential\": stats.expon,\n",
    "    \"Gamma\": stats.gamma,\n",
    "    \"Loglogistic\": stats.fisk,\n",
    "    \"Generalized Pareto\": stats.genpareto\n",
    "}\n",
    "\n",
    "###  Define Probability Array\n",
    "#peak_values = peak_df['peak_va'] #peak_df['peak_va'].dropna()\n",
    "peak_values=df['peak_va']\n",
    "sorted_data = np.sort(peak_values)\n",
    "prob_plot = np.linspace(0.01, 0.99, 100)\n",
    "return_periods = 1 / (1 - prob_plot)\n",
    "prob_exceed = 0.01  # For 100-year flood estimate\n",
    "\n",
    "###  Fit Distributions\n",
    "summary_rows = []\n",
    "fit_results = {}\n",
    "\n",
    "for name, dist in distributions.items():\n",
    "    try:\n",
    "        params = dist.fit(peak_values)\n",
    "        flood_q = dist.ppf(1 - prob_exceed, *params)\n",
    "#        q_estimates = dist.ppf(prob_plot, *params)\n",
    "        # Generate quantiles from same size as data\n",
    "        prob_plot = np.linspace(0.01, 0.99, len(sorted_data))  # now it's 33 points\n",
    "        q_estimates = dist.ppf(prob_plot, *params)\n",
    "\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(sorted_data, q_estimates))\n",
    "        ks_stat, ks_pval = kstest(peak_values, dist.cdf, args=params)\n",
    "\n",
    "        fit_results[name] = {\n",
    "            \"params\": params,\n",
    "            \"q\": dist.ppf(1 - 1 / return_periods, *params),\n",
    "            \"rmse\": rmse,\n",
    "            \"ks_stat\": ks_stat,\n",
    "            \"ks_pval\": ks_pval\n",
    "        }\n",
    "\n",
    "        param_str = \", \".join([f\"{p:.2f}\" for p in params])\n",
    "        summary_rows.append({\n",
    "            \"Distribution\": name,\n",
    "            \"Parameters\": param_str,\n",
    "            \"RMSE (cfs)\": round(rmse, 2),\n",
    "            \"KS Stat\": round(ks_stat, 3),\n",
    "            \"KS p-value\": round(ks_pval, 3),\n",
    "            \"KS Result\": \"✅ Pass\" if ks_pval > 0.05 else \"❌ Reject\"\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not fit {name}: {e}\")\n",
    "\n",
    "### Summary Table\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(by=\"RMSE (cfs)\")\n",
    "print(\"\\n📊 Goodness-of-Fit Summary for All Distributions:\\n\")\n",
    "display(summary_df)\n",
    "\n",
    "### Interactive Plotting\n",
    "def plot_selected_distribution(dist_name):\n",
    "    result = fit_results[dist_name]\n",
    "    q = result[\"q\"]\n",
    "    params = result[\"params\"]\n",
    "    param_str = \", \".join([f\"{p:.2f}\" for p in params])\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(return_periods, q, marker='o', linestyle='-', color='royalblue', label=\"Estimated Peak Flow\")\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel(\"Return Period (years, log scale)\")\n",
    "    plt.ylabel(\"Estimated Peak Flow (cfs)\")\n",
    "    plt.title(f\"{dist_name} Flood Frequency Curve\\nParameters: {param_str}\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Tabular Output\n",
    "    df_plot = pd.DataFrame({\n",
    "        \"Return Period (yr)\": return_periods.round(1),\n",
    "        \"Estimated Peak Flow (cfs)\": q.round(2)\n",
    "    })\n",
    "    print(f\"\\n📌 Parameters: {param_str}\")\n",
    "    print(f\"RMSE: {result['rmse']:.2f}, KS stat: {result['ks_stat']:.3f}, p-value: {result['ks_pval']:.3f}\")\n",
    "    display(df_plot)\n",
    "\n",
    "### Launch Widget\n",
    "interact(plot_selected_distribution, dist_name=Dropdown(options=list(fit_results.keys()), description=\"Distribution\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb9549-b350-4a4a-8183-0883c732983a",
   "metadata": {},
   "source": [
    "## 3. Self-Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c47ac05-66f3-4001-9f83-66350098ecbf",
   "metadata": {},
   "source": [
    "### Self-Assessment: Flood Frequency Analysis Tool\n",
    "\n",
    "Use these prompts and questions to evaluate your understanding of the tool and its underlying hydrologic and statistical concepts.\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Questions\n",
    "\n",
    "1. **Why are return periods plotted on a logarithmic scale in flood frequency analysis?**\n",
    "   - *Hint: Think about how frequent vs. rare events are distributed.*\n",
    "\n",
    "2. **What is the purpose of fitting multiple distributions to the same peak flow dataset?**\n",
    "   - *Hint: No single distribution fits all scenarios equally well.*\n",
    "\n",
    "3. **How do Gringorten plotting positions help in flood frequency analysis?**\n",
    "   - *Hint: They're used to assign empirical probabilities to ordered data.*\n",
    "\n",
    "4. **What assumptions underlie the use of the Gumbel distribution in hydrology?**\n",
    "   - *Hint: It’s designed to model block maxima like annual peak flows.*\n",
    "\n",
    "5. **How do parametric and non-parametric flood frequency methods differ in their approach?**\n",
    "   - *Hint: Consider how the data distribution is treated.*\n",
    "\n",
    "---\n",
    "\n",
    "### Reflective Prompts\n",
    "\n",
    "1. **If two distributions yield similar RMSE but different KS p-values, which metric is more important for selecting a model—and why?**\n",
    "\n",
    "2. **Can a statistically good-fitting distribution be inappropriate for design applications? Provide an example.**\n",
    "\n",
    "3. **How would you adapt this tool to process data from multiple gage stations simultaneously?**\n",
    "\n",
    "4. **What limitations might this tool face when applied to future climate-affected streamflow patterns?**\n",
    "\n",
    "5. **How would the analysis change if you used partial-duration series instead of annual maxima?**\n",
    "\n",
    "---\n",
    "\n",
    "### Quiz Questions\n",
    "\n",
    "**Q1.** The Gumbel distribution is commonly used to model:  \n",
    "A. Rainfall intensity  \n",
    "B. Annual maximum values  \n",
    "C. Median flow durations  \n",
    "D. Baseflow during drought  \n",
    "✅ **Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q2.** The Kolmogorov–Smirnov test compares:  \n",
    "A. Log and normal distributions  \n",
    "B. ECDF and theoretical CDF  \n",
    "C. Mean annual rainfall  \n",
    "D. Number of peaks above threshold  \n",
    "✅ **Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q3.** In the Generalized Extreme Value distribution, the shape parameter controls:  \n",
    "A. Peak discharge  \n",
    "B. Tail behavior  \n",
    "C. Cumulative runoff  \n",
    "D. Frequency of low flows  \n",
    "✅ **Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q4.** A high KS p-value and low RMSE suggest:  \n",
    "A. Overfitting  \n",
    "B. Good model fit  \n",
    "C. Poor data resolution  \n",
    "D. Statistical bias  \n",
    "✅ **Correct:** B\n",
    "\n",
    "---\n",
    "\n",
    "**Q5.** Which distribution is least appropriate for positively skewed hydrologic data?  \n",
    "A. Gumbel  \n",
    "B. Lognormal  \n",
    "C. Normal  \n",
    "D. Log-Pearson III  \n",
    "✅ **Correct:** C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d100d89-68e3-4c92-9cb3-a8f04eec3a5f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "thebe-init"
    ]
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
